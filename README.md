# Deep Q-Network (DQN) Implementation

A comprehensive implementation of Deep Q-Network (DQN) and its variants in PyTorch, featuring real-time training visualization and system monitoring.

## Features

- ðŸ§  Multiple DQN variants:
  - Vanilla DQN
  - Double DQN
  - Dueling DQN
- ðŸ“Š Real-time training visualization:
  - Training statistics
  - Progress tracking
  - System resource monitoring (CPU/GPU usage)
- ðŸ”„ Experience replay buffer
- ðŸŽ¯ Target network for stable training
- ðŸ“ˆ Automatic model checkpointing
- ðŸ“‰ Training history plotting
- ðŸŽ® Support for various Gymnasium environments

## Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/DQN.git
cd DQN
```

2. Create and activate a virtual environment (optional but recommended):

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

## Project Structure

```
DQN/
â”œâ”€â”€ dqn/                    # Main package directory
â”‚   â”œâ”€â”€ agents/            # DQN agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â”‚   â”œâ”€â”€ double_dqn_agent.py
â”‚   â”‚   â””â”€â”€ dueling_dqn_agent.py
â”‚   â”œâ”€â”€ configs/           # Configuration files
â”‚   â”‚   â””â”€â”€ default_config.py
â”‚   â”œâ”€â”€ models/            # Neural network architectures
â”‚   â”‚   â””â”€â”€ q_network.py
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”œâ”€â”€ plot_utils.py
â”‚   â”‚   â””â”€â”€ replay_buffer.py
â”‚   â”œâ”€â”€ train.py          # Training script
â”‚   â””â”€â”€ evaluate.py       # Evaluation script
â”œâ”€â”€ requirements.txt       # Project dependencies
â”œâ”€â”€ setup.py              # Package setup file
â””â”€â”€ README.md             # This file
```

## Usage

### Training

To train a DQN agent:

```bash
python -m dqn.train --agent [dqn|double_dqn|dueling_dqn] --env [environment_name]
```

Example:

```bash
python -m dqn.train --agent dueling_dqn --env CartPole-v1
```

### Evaluation

To evaluate a trained agent:

```bash
python -m dqn.evaluate [model_path] --env [environment_name] --episodes [num_episodes]
```

Example:

```bash
python -m dqn.evaluate saved_models/best_dueling_dqn_model.pth --env CartPole-v1 --episodes 10
```

## Configuration

You can modify the training parameters in `dqn/configs/default_config.py`. Key parameters include:

- Learning rate
- Batch size
- Memory size
- Discount factor (gamma)
- Epsilon parameters for exploration
- Network architecture
- Training intervals

## Training Visualization

The training script provides real-time visualization with:

- Left panel:
  - Current episode progress
  - Episode rewards
  - Moving average reward
  - Best reward achieved
  - Latest loss value

- Right panel:
  - Training progress bar
  - CPU usage
  - Memory usage
  - GPU usage and memory (if available)

## Results

Models are automatically saved during training:

- `best_{agent_type}_model.pth`: Best performing model
- `latest_{agent_type}_model.pth`: Most recent model
- `final_{agent_type}_model.pth`: Model after training completion

Training plots are saved in the specified output directory, showing:

- Episode rewards
- Moving average rewards
- Evaluation rewards

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- [DeepMind's DQN Paper](https://www.nature.com/articles/nature14236)
- [OpenAI's Gymnasium](https://gymnasium.farama.org/)
- [PyTorch](https://pytorch.org/)
- [Rich](https://rich.readthedocs.io/) for terminal visualization
